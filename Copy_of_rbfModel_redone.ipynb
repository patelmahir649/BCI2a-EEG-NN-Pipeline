{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patelmahir649/BCI2a-EEG-NN-Pipeline/blob/main/Copy_of_rbfModel_redone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TV4zdNOGxLGC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, TensorDataset as TData\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "import pickle\n",
        "import zipfile\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split as tts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "\n",
        "import optuna\n",
        "from optuna.trial import TrialState"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nG6BVC9WQvWT",
        "outputId": "d466f95a-7907-4200-8d79-ddb2f705636f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.15.2 colorlog-6.9.0 optuna-4.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87rL1bRKxLGD",
        "outputId": "d552095a-f1a6-417a-9b5f-6c3b5d91f762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  LHNT_Individual_Data.zip\n",
            "  inflating: LHNT EEG/Alan_Fletcher_Session4.zip  \n",
            "  inflating: LHNT EEG/Ademola_Adetosoye_Session1.0.zip  \n",
            "  inflating: LHNT EEG/Alex_Xie_Session1.zip  \n",
            "  inflating: LHNT EEG/Alex_Xie_Session2.zip  \n",
            "  inflating: LHNT EEG/Alan_Fletcher_Session2.zip  \n",
            "  inflating: LHNT EEG/Alex_Xie_Session3.zip  \n",
            "  inflating: LHNT EEG/Alan_Fletcher_Session1.zip  \n",
            "  inflating: LHNT EEG/Alan_Fletcher_Session3.zip  \n",
            "  inflating: LHNT EEG/Alex_Xie_Session4.zip  \n",
            "  inflating: LHNT EEG/Jane_Doe_Session407.0.zip  \n",
            "  inflating: LHNT EEG/jimmy_neutron_Session3.0.zip  \n",
            "  inflating: LHNT EEG/jimmy_neutron_Session4.0.zip  \n",
            "  inflating: LHNT EEG/Maddox_Fletcher_Session1.0.zip  \n",
            "  inflating: LHNT EEG/morgan_dye_Session1.zip  \n",
            "  inflating: LHNT EEG/morgan_dye_Session2.zip  \n",
            "  inflating: LHNT EEG/morgan_dye_Session3.zip  \n",
            "  inflating: LHNT EEG/nandini_senthilkumar_Session1.zip  \n",
            "  inflating: LHNT EEG/nandini_senthilkumar_Session2.zip  \n",
            "  inflating: LHNT EEG/nandini_senthilkumar_Session3.zip  \n",
            "  inflating: LHNT EEG/nandini_senthilkumar_Session4.zip  \n",
            "  inflating: LHNT EEG/Nandini_Senthilkumar_Session5.zip  \n",
            "  inflating: LHNT EEG/Nandini_Senthilkumar_Session6.zip  \n",
            "  inflating: LHNT EEG/Samarth_Rao_Session1.0.zip  \n",
            "  inflating: LHNT EEG/Samarth_Rao_Session2.0.zip  \n",
            "  inflating: LHNT EEG/Samarth_Rao_Session3.0.zip  \n",
            "  inflating: LHNT EEG/Samarth_Rao_Session4.0.zip  \n",
            "  inflating: LHNT EEG/Samarth_Rao_Session5.0.zip  \n",
            "  inflating: LHNT EEG/user_table.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip \"LHNT_Individual_Data.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "s35Xuf4exLGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17bce0fe-052b-4683-b90a-045118adb261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 0 Total labels: 0\n"
          ]
        }
      ],
      "source": [
        "def getAllPickles(directory=\"LHNT EEG\"):\n",
        "    # List all subfolders and collect paths to all .pkl files.\n",
        "    folders = [drctry for drctry in os.listdir(directory) if os.path.isdir(os.path.join(directory, drctry))]\n",
        "    files = []\n",
        "    for folder in folders:\n",
        "        folder_files = os.listdir(os.path.join(directory, folder))\n",
        "        for file in folder_files:\n",
        "            if \".pkl\" in file:\n",
        "                files.append(os.path.join(directory, folder, file))\n",
        "    return files\n",
        "\n",
        "def npFromPickle(pickle_files):\n",
        "    # Load each pickle file and assign a label:\n",
        "    # 1 if filename contains \"right\", else 0.\n",
        "    np_data = []\n",
        "    labels = []  # 0 is left, 1 is right\n",
        "    for file in pickle_files:\n",
        "        with open(file, \"rb\") as f:\n",
        "            data1 = pickle.load(f)\n",
        "            np_data.append(data1[0])\n",
        "        if 'right' in file.split('/')[-1]:\n",
        "            labels.append(1)\n",
        "        else:\n",
        "            labels.append(0)\n",
        "    return np_data, labels\n",
        "\n",
        "np_data, labels = npFromPickle(getAllPickles())\n",
        "print(\"Total samples:\", len(np_data), \"Total labels:\", len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "roLkiNx6xLGF"
      },
      "outputs": [],
      "source": [
        "def bandpass_filter(signal, crit_freq=[1, 40], sampling_freq=125, plot=False, channel=0):\n",
        "    # Applies a Butterworth bandpass filter.\n",
        "    order = 4\n",
        "    b, a = scipy.signal.butter(order, crit_freq, btype='bandpass', fs=sampling_freq)\n",
        "    processed_signal = scipy.signal.filtfilt(b, a, signal, 1)\n",
        "    if plot:\n",
        "        plt.figure()\n",
        "        plt.xlabel('Time')\n",
        "        plt.ylabel(f'Normalized amplitude of channel {channel}')\n",
        "        plt.title(f'{crit_freq[0]}-{crit_freq[1]}Hz bandpass filter')\n",
        "        signal_min = np.full((signal.shape[1], signal.shape[0]), np.min(signal, 1)).transpose()\n",
        "        signal_max = np.full((signal.shape[1], signal.shape[0]), np.max(signal, 1)).transpose()\n",
        "        normed_signal = (signal - signal_min) / (signal_max - signal_min)\n",
        "        filtered_min = np.full((processed_signal.shape[1], processed_signal.shape[0]), np.min(processed_signal, 1)).transpose()\n",
        "        filtered_max = np.full((processed_signal.shape[1], processed_signal.shape[0]), np.max(processed_signal, 1)).transpose()\n",
        "        normed_filt = (processed_signal - filtered_min) / (filtered_max - filtered_min)\n",
        "        plt.plot(np.arange(normed_signal[channel].size), normed_signal[channel], label='Input')\n",
        "        plt.plot(np.arange(normed_filt[channel].size), normed_filt[channel], label='Transformed')\n",
        "        plt.legend()\n",
        "    return processed_signal\n",
        "\n",
        "def segmentation(signal, sampling_freq=125, window_size=1, window_shift=0.016):\n",
        "    # Segments the signal into overlapping windows.\n",
        "    w_size = int(sampling_freq * window_size)\n",
        "    w_shift = int(sampling_freq * window_shift)\n",
        "    segments = []\n",
        "    i = 0\n",
        "    while i + w_size <= signal.shape[1]:\n",
        "        segments.append(signal[:, i: i + w_size])\n",
        "        i += w_shift\n",
        "    return segments\n",
        "\n",
        "def channel_rearrangment(sig, channel_order):\n",
        "    # Rearranges channels according to the provided order.\n",
        "    # The channel_order is given in 1-indexed format.\n",
        "    channel_order = [channel - 1 for channel in channel_order]\n",
        "    reindexed = np.zeros_like(sig)\n",
        "    for i, ind in enumerate(channel_order):\n",
        "        reindexed[i] = sig[ind]\n",
        "    return reindexed\n",
        "\n",
        "# Define desired channel order.\n",
        "ordered_channels = [1, 9, 11, 3, 2, 12, 10, 4, 13, 5, 15, 7, 14, 16, 6, 8]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FixedGammaRBFLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, gamma=0.1):\n",
        "        super(FixedGammaRBFLayer, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.gamma = gamma\n",
        "        self.centers = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.weights = nn.Parameter(torch.ones(out_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.centers)\n",
        "        nn.init.constant_(self.weights, 1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, in_features)\n",
        "        x_expanded = x.unsqueeze(1)                 # (batch, 1, in_features)\n",
        "        centers_expanded = self.centers.unsqueeze(0)  # (1, out_features, in_features)\n",
        "        diff = x_expanded - centers_expanded\n",
        "        dist_sq = torch.sum(diff ** 2, dim=2)         # (batch, out_features)\n",
        "        rbf_out = torch.exp(-self.gamma * dist_sq)    # RBF activation.\n",
        "        rbf_out = rbf_out * self.weights              # Scale by trainable weights.\n",
        "        return rbf_out\n",
        "\n",
        "class RBFNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, num_rbf_units, num_classes,\n",
        "                 gamma=0.1, hidden_dim=512, dropout_prob=0.5):\n",
        "        super(RBFNetwork, self).__init__()\n",
        "        # RBF layer: transforms flattened input into RBF activations.\n",
        "        self.rbf = FixedGammaRBFLayer(input_dim, num_rbf_units, gamma=gamma)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        # Classifier: two-layer FC network with ReLU and dropout.\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(num_rbf_units, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten input: (batch, channels, sequence_length) -> (batch, input_dim)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x_rbf = self.rbf(x)\n",
        "        out = self.fc(x_rbf)\n",
        "        return out\n",
        "\n",
        "def initialize_rbf_centers(model, data_loader, num_samples=1000):\n",
        "    # Use KMeans clustering on a subset of training data to initialize RBF centers.\n",
        "    model.eval()\n",
        "    all_features = []\n",
        "    total_samples = 0\n",
        "    for features, _ in data_loader:\n",
        "        features = features.to(device)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        with torch.no_grad():\n",
        "            all_features.append(features.cpu())\n",
        "        total_samples += features.size(0)\n",
        "        if total_samples >= num_samples:\n",
        "            break\n",
        "    all_features = torch.cat(all_features, dim=0)\n",
        "    kmeans = KMeans(n_clusters=model.rbf.out_features, random_state=0)\n",
        "    kmeans.fit(all_features.numpy())\n",
        "    centers = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32, device=device)\n",
        "    model.rbf.centers.data.copy_(centers)\n",
        "    print(\"RBF centers initialized using KMeans on input features.\")\n",
        "\n",
        "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=20, patience=5):\n",
        "    best_valid_accuracy = 0.0\n",
        "    no_improve_epochs = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for features, labels in train_loader:\n",
        "            features = features.to(device)\n",
        "            labels = labels.argmax(dim=1).to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for features, labels in valid_loader:\n",
        "                features = features.to(device)\n",
        "                labels = labels.argmax(dim=1).to(device)\n",
        "                outputs = model(features)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        valid_accuracy = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Valid Accuracy: {valid_accuracy:.2f}%\")\n",
        "\n",
        "        # Early stopping check\n",
        "        if valid_accuracy > best_valid_accuracy:\n",
        "            best_valid_accuracy = valid_accuracy\n",
        "            no_improve_epochs = 0\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "            if no_improve_epochs >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    return best_valid_accuracy\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for features, labels in data_loader:\n",
        "            features = features.to(device)\n",
        "            labels = labels.argmax(dim=1).to(device)\n",
        "            outputs = model(features)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy\n",
        "\n",
        "def objective(trial):\n",
        "    # Hyperparameters to optimize\n",
        "    num_rbf_units = trial.suggest_int(\"num_rbf_units\", 64, 512, step=64)\n",
        "    gamma = trial.suggest_float(\"gamma\", 0.001, 1.0, log=True)\n",
        "    hidden_dim = trial.suggest_int(\"hidden_dim\", 128, 1024, step=128)\n",
        "    dropout_prob = trial.suggest_float(\"dropout_prob\", 0.1, 0.7)\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
        "\n",
        "    # Create DataLoaders with the suggested batch size\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize model with the suggested hyperparameters\n",
        "    model = RBFNetwork(\n",
        "        input_dim=input_dim,\n",
        "        num_rbf_units=num_rbf_units,\n",
        "        num_classes=num_classes,\n",
        "        gamma=gamma,\n",
        "        hidden_dim=hidden_dim,\n",
        "        dropout_prob=dropout_prob\n",
        "    ).to(device)\n",
        "\n",
        "    # Initialize RBF centers using KMeans on training data\n",
        "    initialize_rbf_centers(model, train_loader, num_samples=500)\n",
        "\n",
        "    # Setup optimizer with suggested learning rate and weight decay\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Train and validate the model\n",
        "    valid_accuracy = train_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        valid_loader=valid_loader,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        num_epochs=20,\n",
        "        patience=5\n",
        "    )\n",
        "\n",
        "    return valid_accuracy"
      ],
      "metadata": {
        "id": "im6bUmT1UKzR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gY_-bWQCxLGG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da8b5a17-0d31-4ca9-da22-c967c526c4e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ZIP files...\n",
            "Extracted: Alex_Xie_Session2.zip\n",
            "Extracted: Samarth_Rao_Session5.0.zip\n",
            "Extracted: jimmy_neutron_Session4.0.zip\n",
            "Extracted: morgan_dye_Session1.zip\n",
            "Extracted: nandini_senthilkumar_Session4.zip\n",
            "Extracted: morgan_dye_Session2.zip\n",
            "Extracted: morgan_dye_Session3.zip\n",
            "Extracted: Samarth_Rao_Session3.0.zip\n",
            "Extracted: Samarth_Rao_Session1.0.zip\n",
            "Extracted: Nandini_Senthilkumar_Session6.zip\n",
            "Extracted: Jane_Doe_Session407.0.zip\n",
            "Extracted: Alan_Fletcher_Session2.zip\n",
            "Extracted: Samarth_Rao_Session2.0.zip\n",
            "Extracted: Ademola_Adetosoye_Session1.0.zip\n",
            "Extracted: jimmy_neutron_Session3.0.zip\n",
            "Extracted: Alex_Xie_Session1.zip\n",
            "Extracted: nandini_senthilkumar_Session1.zip\n",
            "Extracted: Alan_Fletcher_Session4.zip\n",
            "Extracted: Alan_Fletcher_Session1.zip\n",
            "Extracted: Samarth_Rao_Session4.0.zip\n",
            "Extracted: Alex_Xie_Session4.zip\n",
            "Extracted: nandini_senthilkumar_Session2.zip\n",
            "Extracted: Alex_Xie_Session3.zip\n",
            "Extracted: Maddox_Fletcher_Session1.0.zip\n",
            "Extracted: Nandini_Senthilkumar_Session5.zip\n",
            "Extracted: Alan_Fletcher_Session3.zip\n",
            "Extracted: nandini_senthilkumar_Session3.zip\n",
            "\n",
            "Extracted 27 ZIP files\n",
            "Found 397 pickle files\n",
            "Found 2 different shapes in the data\n",
            "Shape (16, 875): 340 samples\n",
            "Shape (16, 1750): 57 samples\n",
            "Loaded 397 samples with 397 labels\n",
            "Most common shape: (16, 875)\n",
            "After filtering: 340 samples with consistent shape\n",
            "Data array shape: (340, 16, 875)\n",
            "\n",
            "Split data:\n",
            "Train: (255, 16, 875) with 123 right and 132 left samples\n",
            "Validation: (42, 16, 875) with 24 right and 18 left samples\n",
            "Test: (43, 16, 875) with 23 right and 20 left samples\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "import shutil\n",
        "\n",
        "# Create a directory for extracting the zip files\n",
        "extract_dir = \"extracted_eeg_data\"\n",
        "if os.path.exists(extract_dir):\n",
        "    shutil.rmtree(extract_dir)  # Remove if exists to start fresh\n",
        "os.makedirs(extract_dir)\n",
        "\n",
        "# Extract all zip files\n",
        "print(\"Extracting ZIP files...\")\n",
        "zip_count = 0\n",
        "for zip_file in os.listdir('LHNT EEG'):\n",
        "    if zip_file.endswith('.zip'):\n",
        "        zip_count += 1\n",
        "        zip_path = os.path.join('LHNT EEG', zip_file)\n",
        "        # Create a specific directory for each zip file to avoid file collisions\n",
        "        session_extract_dir = os.path.join(extract_dir, os.path.splitext(zip_file)[0])\n",
        "        os.makedirs(session_extract_dir, exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(session_extract_dir)\n",
        "            print(f\"Extracted: {zip_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting {zip_file}: {e}\")\n",
        "\n",
        "print(f\"\\nExtracted {zip_count} ZIP files\")\n",
        "\n",
        "# Now look for pickle files in the extracted directories\n",
        "def getAllPickles(directory):\n",
        "    \"\"\"Find all pickle files in directory and subdirectories.\"\"\"\n",
        "    all_files = []\n",
        "\n",
        "    # Walk through all subdirectories\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        if \"__MACOSX\" in root:  # Skip macOS system directories\n",
        "            continue\n",
        "\n",
        "        for file in files:\n",
        "            if file.endswith(\".pkl\") and not file.startswith(\"._\"):\n",
        "                all_files.append(os.path.join(root, file))\n",
        "\n",
        "    print(f\"Found {len(all_files)} pickle files\")\n",
        "    return all_files\n",
        "\n",
        "def npFromPickle(pickle_files, debug=True):\n",
        "    \"\"\"Load data from pickle files and return data, labels and shapes.\"\"\"\n",
        "    np_data = []\n",
        "    labels = [] # 0 is left, 1 is right\n",
        "    shapes = []\n",
        "\n",
        "    for file in pickle_files:\n",
        "        if \"__MACOSX\" in file:  # Skip macOS system files\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with open(file, \"rb\") as f:\n",
        "                data1 = pickle.load(f)\n",
        "                np_data.append(data1[0])\n",
        "                shapes.append(data1[0].shape)\n",
        "\n",
        "            # Check for 'right' or 'left' in filename\n",
        "            filename = os.path.basename(file).lower()\n",
        "            if 'right' in filename:\n",
        "                labels.append(1)\n",
        "            elif 'left' in filename:\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                print(f\"Warning: Cannot determine left/right from filename: {file}\")\n",
        "                # Skip this file or use another method to determine label\n",
        "                np_data.pop()  # Remove the corresponding data\n",
        "                shapes.pop()   # Remove the corresponding shape\n",
        "                continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "    if debug and shapes:\n",
        "        # Find common shapes\n",
        "        from collections import Counter\n",
        "        shape_counts = Counter(shapes)\n",
        "        print(f\"Found {len(shape_counts)} different shapes in the data\")\n",
        "        for shape, count in shape_counts.most_common(5):\n",
        "            print(f\"Shape {shape}: {count} samples\")\n",
        "\n",
        "    return np_data, labels, shapes\n",
        "\n",
        "# Get pickle files from extracted directories\n",
        "pickle_files = getAllPickles(extract_dir)\n",
        "\n",
        "if not pickle_files:\n",
        "    print(\"No pickle files found in extracted directories.\")\n",
        "\n",
        "    # Let's look for any files to understand the structure\n",
        "    print(\"\\nExamining extracted directory structure:\")\n",
        "    for root, dirs, files in os.walk(extract_dir):\n",
        "        if files:\n",
        "            print(f\"Directory: {root}\")\n",
        "            # Show first 5 files in each directory with files\n",
        "            for file in files[:5]:\n",
        "                print(f\"  - {file}\")\n",
        "            if len(files) > 5:\n",
        "                print(f\"  - ... and {len(files)-5} more files\")\n",
        "\n",
        "    print(\"\\nFile extensions in extracted data:\")\n",
        "    extensions = {}\n",
        "    for root, dirs, files in os.walk(extract_dir):\n",
        "        for file in files:\n",
        "            ext = os.path.splitext(file)[1].lower()\n",
        "            extensions[ext] = extensions.get(ext, 0) + 1\n",
        "\n",
        "    for ext, count in extensions.items():\n",
        "        print(f\"{ext}: {count} files\")\n",
        "\n",
        "else:\n",
        "    # Process and load the data\n",
        "    np_data, labels, shapes = npFromPickle(pickle_files)\n",
        "    print(f\"Loaded {len(np_data)} samples with {len(labels)} labels\")\n",
        "\n",
        "    if not shapes or len(shapes) == 0:\n",
        "        print(\"No valid data shapes found. Check your pickle files.\")\n",
        "    else:\n",
        "        # Now let's handle the different shapes\n",
        "        # Option 1: Keep only samples with the most common shape\n",
        "        from collections import Counter\n",
        "        shape_counts = Counter(shapes)\n",
        "        most_common_shape = shape_counts.most_common(1)[0][0]\n",
        "        print(f\"Most common shape: {most_common_shape}\")\n",
        "\n",
        "        # Filter data to only include samples with the most common shape\n",
        "        filtered_indices = [i for i, shape in enumerate(shapes) if shape == most_common_shape]\n",
        "        filtered_data = [np_data[i] for i in filtered_indices]\n",
        "        filtered_labels = [labels[i] for i in filtered_indices]\n",
        "\n",
        "        print(f\"After filtering: {len(filtered_data)} samples with consistent shape\")\n",
        "\n",
        "        # Convert filtered data to numpy arrays\n",
        "        np_data_array = np.array(filtered_data)\n",
        "        np_labels = np.array(filtered_labels)\n",
        "\n",
        "        print(f\"Data array shape: {np_data_array.shape}\")\n",
        "\n",
        "        # Now do the train-test split\n",
        "        train_x, test_x, train_y, test_y = tts(np_data_array, np_labels, test_size=0.25, random_state=42)\n",
        "        val_x, test_x = test_x[:len(test_x)//2], test_x[len(test_x)//2:]\n",
        "        val_y, test_y = test_y[:len(test_y)//2], test_y[len(test_y)//2:]\n",
        "\n",
        "        print(f\"\\nSplit data:\")\n",
        "        print(f\"Train: {train_x.shape} with {sum(train_y)} right and {len(train_y) - sum(train_y)} left samples\")\n",
        "        print(f\"Validation: {val_x.shape} with {sum(val_y)} right and {len(val_y) - sum(val_y)} left samples\")\n",
        "        print(f\"Test: {test_x.shape} with {sum(test_y)} right and {len(test_y) - sum(test_y)} left samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "RJ9ASJC8xLGG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fa29bb67-cfcc-49fd-84bd-fb1352718461"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-26 23:02:11,329] A new study created in memory with name: rbf_network_optimization\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "RBF centers initialized using KMeans on input features.\n",
            "Epoch 1/20 | Train Loss: 0.6930 | Valid Accuracy: 42.86%\n",
            "Epoch 2/20 | Train Loss: 0.6929 | Valid Accuracy: 42.86%\n",
            "Epoch 3/20 | Train Loss: 0.6928 | Valid Accuracy: 42.86%\n",
            "Epoch 4/20 | Train Loss: 0.6929 | Valid Accuracy: 42.86%\n",
            "Epoch 5/20 | Train Loss: 0.6927 | Valid Accuracy: 42.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-26 23:03:02,679] Trial 0 finished with value: 42.857142857142854 and parameters: {'num_rbf_units': 128, 'gamma': 0.07753921665396608, 'hidden_dim': 1024, 'dropout_prob': 0.6899455042489528, 'lr': 0.0005228299729168187, 'weight_decay': 8.026967328195932e-05, 'batch_size': 128}. Best is trial 0 with value: 42.857142857142854.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20 | Train Loss: 0.6928 | Valid Accuracy: 42.86%\n",
            "Early stopping at epoch 6\n",
            "RBF centers initialized using KMeans on input features.\n",
            "Epoch 1/20 | Train Loss: 0.6943 | Valid Accuracy: 42.86%\n",
            "Epoch 2/20 | Train Loss: 0.6931 | Valid Accuracy: 42.86%\n",
            "Epoch 3/20 | Train Loss: 0.6933 | Valid Accuracy: 42.86%\n",
            "Epoch 4/20 | Train Loss: 0.6934 | Valid Accuracy: 42.86%\n",
            "Epoch 5/20 | Train Loss: 0.6934 | Valid Accuracy: 42.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-26 23:03:34,436] Trial 1 finished with value: 42.857142857142854 and parameters: {'num_rbf_units': 64, 'gamma': 0.005576329386090816, 'hidden_dim': 1024, 'dropout_prob': 0.6980811904720657, 'lr': 0.008916157870479833, 'weight_decay': 0.00028541510866998183, 'batch_size': 64}. Best is trial 0 with value: 42.857142857142854.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20 | Train Loss: 0.6935 | Valid Accuracy: 42.86%\n",
            "Early stopping at epoch 6\n",
            "RBF centers initialized using KMeans on input features.\n",
            "Epoch 1/20 | Train Loss: 0.6925 | Valid Accuracy: 42.86%\n",
            "Epoch 2/20 | Train Loss: 0.6926 | Valid Accuracy: 42.86%\n",
            "Epoch 3/20 | Train Loss: 0.6925 | Valid Accuracy: 42.86%\n",
            "Epoch 4/20 | Train Loss: 0.6925 | Valid Accuracy: 42.86%\n",
            "Epoch 5/20 | Train Loss: 0.6926 | Valid Accuracy: 42.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-26 23:05:42,826] Trial 2 finished with value: 42.857142857142854 and parameters: {'num_rbf_units': 384, 'gamma': 0.07036171090090922, 'hidden_dim': 768, 'dropout_prob': 0.26317007976214823, 'lr': 0.00010253247109351304, 'weight_decay': 0.0009481378735028546, 'batch_size': 64}. Best is trial 0 with value: 42.857142857142854.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20 | Train Loss: 0.6926 | Valid Accuracy: 42.86%\n",
            "Early stopping at epoch 6\n",
            "RBF centers initialized using KMeans on input features.\n",
            "Epoch 1/20 | Train Loss: 0.6930 | Valid Accuracy: 42.86%\n",
            "Epoch 2/20 | Train Loss: 0.6928 | Valid Accuracy: 42.86%\n",
            "Epoch 3/20 | Train Loss: 0.6928 | Valid Accuracy: 42.86%\n",
            "Epoch 4/20 | Train Loss: 0.6929 | Valid Accuracy: 42.86%\n",
            "Epoch 5/20 | Train Loss: 0.6928 | Valid Accuracy: 42.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-04-26 23:07:34,971] Trial 3 failed with parameters: {'num_rbf_units': 320, 'gamma': 0.01420204644482993, 'hidden_dim': 896, 'dropout_prob': 0.3982240551447057, 'lr': 0.0035244201654876764, 'weight_decay': 2.308145741787735e-05, 'batch_size': 32} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-10-377153e2782d>\", line 162, in objective\n",
            "    valid_accuracy = train_model(\n",
            "                     ^^^^^^^^^^^^\n",
            "  File \"<ipython-input-10-377153e2782d>\", line 83, in train_model\n",
            "    train_loss += loss.item()\n",
            "                  ^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-04-26 23:07:34,973] Trial 3 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-ce06b4041e8d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m                            \u001b[0mstudy_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rbf_network_optimization\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                            pruner=optuna.pruners.MedianPruner())\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Number of trials for hyperparameter search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# Print optimization results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-377153e2782d>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;31m# Train and validate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     valid_accuracy = train_model(\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-377153e2782d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, valid_loader, criterion, optimizer, num_epochs, patience)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "    # Preprocess and segment data\n",
        "    train_eeg = []\n",
        "    train_labels = []\n",
        "    valid_eeg = []\n",
        "    valid_labels = []\n",
        "    test_eeg = []\n",
        "    test_labels = []\n",
        "\n",
        "    for sig, label in zip(train_x, train_y):\n",
        "        if sig.shape[1] == 0:\n",
        "            continue\n",
        "        reindexed_signal = channel_rearrangment(sig, ordered_channels)\n",
        "        filtered_sig = bandpass_filter(reindexed_signal, [5, 40], 125)\n",
        "        normed_sig = (filtered_sig - np.mean(filtered_sig, 1, keepdims=True)) / np.std(filtered_sig, 1, keepdims=True)\n",
        "        if np.isnan(normed_sig).any():\n",
        "            continue\n",
        "        signals = segmentation(normed_sig, 125, window_size=1.5, window_shift=0.0175)\n",
        "        train_eeg.extend(signals)\n",
        "        train_labels.extend([label] * len(signals))\n",
        "\n",
        "    for sig, label in zip(val_x, val_y):\n",
        "        if sig.shape[1] == 0:\n",
        "            continue\n",
        "        reindexed_signal = channel_rearrangment(sig, ordered_channels)\n",
        "        filtered_sig = bandpass_filter(reindexed_signal, [5, 40], 125)\n",
        "        normed_sig = (filtered_sig - np.mean(filtered_sig, 1, keepdims=True)) / np.std(filtered_sig, 1, keepdims=True)\n",
        "        if np.isnan(normed_sig).any():\n",
        "            continue\n",
        "        signals = segmentation(normed_sig, 125, window_size=1.5, window_shift=0.0175)\n",
        "        valid_eeg.extend(signals)\n",
        "        valid_labels.extend([label] * len(signals))\n",
        "\n",
        "    for sig, label in zip(test_x, test_y):\n",
        "        if sig.shape[1] == 0:\n",
        "            continue\n",
        "        reindexed_signal = channel_rearrangment(sig, ordered_channels)\n",
        "        filtered_sig = bandpass_filter(reindexed_signal, [5, 40], 125)\n",
        "        normed_sig = (filtered_sig - np.mean(filtered_sig, 1, keepdims=True)) / np.std(filtered_sig, 1, keepdims=True)\n",
        "        if np.isnan(normed_sig).any():\n",
        "            continue\n",
        "        signals = segmentation(normed_sig, 125, window_size=1.5, window_shift=0.0175)\n",
        "        test_eeg.extend(signals)\n",
        "        test_labels.extend([label] * len(signals))\n",
        "\n",
        "    # Remove some channels to reduce dimensionality\n",
        "    columns_to_remove = [1, 2, 7, 8]\n",
        "    train_eeg = [np.delete(arr, columns_to_remove, axis=0) for arr in train_eeg]\n",
        "    valid_eeg = [np.delete(arr, columns_to_remove, axis=0) for arr in valid_eeg]\n",
        "    test_eeg = [np.delete(arr, columns_to_remove, axis=0) for arr in test_eeg]\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    train_eeg_tensor = torch.zeros((len(train_eeg), train_eeg[0].shape[0], train_eeg[0].shape[1]))\n",
        "    valid_eeg_tensor = torch.zeros((len(valid_eeg), valid_eeg[0].shape[0], valid_eeg[0].shape[1]))\n",
        "    test_eeg_tensor = torch.zeros((len(test_eeg), test_eeg[0].shape[0], test_eeg[0].shape[1]))\n",
        "\n",
        "    for i in range(len(train_eeg)):\n",
        "        train_eeg_tensor[i] = torch.from_numpy(train_eeg[i].copy())\n",
        "    for i in range(len(valid_eeg)):\n",
        "        valid_eeg_tensor[i] = torch.from_numpy(valid_eeg[i].copy())\n",
        "    for i in range(len(test_eeg)):\n",
        "        test_eeg_tensor[i] = torch.from_numpy(test_eeg[i].copy())\n",
        "\n",
        "    # Create one-hot encoded labels\n",
        "    num_classes = 2\n",
        "    train_label_tensor = torch.zeros(len(train_labels), num_classes)\n",
        "    valid_label_tensor = torch.zeros(len(valid_labels), num_classes)\n",
        "    test_label_tensor = torch.zeros(len(test_labels), num_classes)\n",
        "\n",
        "    for i, val in enumerate(train_labels):\n",
        "        train_label_tensor[i][val] = 1\n",
        "    for i, val in enumerate(valid_labels):\n",
        "        valid_label_tensor[i][val] = 1\n",
        "    for i, val in enumerate(test_labels):\n",
        "        test_label_tensor[i][val] = 1\n",
        "\n",
        "    # Create TensorDatasets\n",
        "    train_ds = TData(train_eeg_tensor, train_label_tensor)\n",
        "    valid_ds = TData(valid_eeg_tensor, valid_label_tensor)\n",
        "    test_ds = TData(test_eeg_tensor, test_label_tensor)\n",
        "\n",
        "    # Determine input dimensions and number of classes\n",
        "    sample_feature, sample_label = train_ds[0]\n",
        "    input_dim = sample_feature.numel()  # Flattened input dimension\n",
        "    num_classes = sample_label.shape[0]  # Number of classes\n",
        "\n",
        "    # Use GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Set up Optuna study for hyperparameter optimization\n",
        "    study = optuna.create_study(direction=\"maximize\",\n",
        "                               study_name=\"rbf_network_optimization\",\n",
        "                               pruner=optuna.pruners.MedianPruner())\n",
        "    study.optimize(objective, n_trials=50)  # Number of trials for hyperparameter search\n",
        "\n",
        "    # Print optimization results\n",
        "    print(\"Number of finished trials: \", len(study.trials))\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print(f\"  Value: {trial.value}\")\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(f\"    {key}: {value}\")\n",
        "\n",
        "    # Train final model with best hyperparameters\n",
        "    print(\"\\nTraining final model with best hyperparameters...\")\n",
        "\n",
        "    # Create DataLoaders for the final model\n",
        "    final_batch_size = trial.params[\"batch_size\"]\n",
        "    train_loader = DataLoader(train_ds, batch_size=final_batch_size, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_ds, batch_size=final_batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_ds, batch_size=final_batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize the best model\n",
        "    best_model = RBFNetwork(\n",
        "        input_dim=input_dim,\n",
        "        num_rbf_units=trial.params[\"num_rbf_units\"],\n",
        "        num_classes=num_classes,\n",
        "        gamma=trial.params[\"gamma\"],\n",
        "        hidden_dim=trial.params[\"hidden_dim\"],\n",
        "        dropout_prob=trial.params[\"dropout_prob\"]\n",
        "    ).to(device)\n",
        "\n",
        "    # Initialize RBF centers\n",
        "    initialize_rbf_centers(best_model, train_loader, num_samples=500)\n",
        "\n",
        "    # Set up optimizer and criterion\n",
        "    best_optimizer = optim.Adam(\n",
        "        best_model.parameters(),\n",
        "        lr=trial.params[\"lr\"],\n",
        "        weight_decay=trial.params[\"weight_decay\"]\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Train the final model\n",
        "    train_model(\n",
        "        model=best_model,\n",
        "        train_loader=train_loader,\n",
        "        valid_loader=valid_loader,\n",
        "        criterion=criterion,\n",
        "        optimizer=best_optimizer,\n",
        "        num_epochs=30,  # Train for longer for the final model\n",
        "        patience=10\n",
        "    )\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    print(\"\\nValidation Set Evaluation:\")\n",
        "    valid_accuracy = evaluate_model(best_model, valid_loader)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(\"\\nTest Set Evaluation:\")\n",
        "    test_accuracy = evaluate_model(best_model, test_loader)\n",
        "\n",
        "    # Save the best model\n",
        "    torch.save(best_model.state_dict(), \"best_rbf_network.pth\")\n",
        "    print(\"Best model saved to best_rbf_network.pth\")\n",
        "\n",
        "    # Save the study\n",
        "    import joblib\n",
        "    joblib.dump(study, \"optuna_study.pkl\")\n",
        "    print(\"Optuna study saved to optuna_study.pkl\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}